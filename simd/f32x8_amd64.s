// Code generated by command: go run gen_amd64.go -out D:/projects/quant1x/x/simd/f32x8_amd64.s -stubs D:/projects/quant1x/x/simd/f32x8_amd64.go -pkg simd. DO NOT EDIT.

#include "textflag.h"

// func f32x8_add(a []float32, b []float32, result []float32) int
// Requires: AVX
TEXT ·f32x8_add(SB), NOSPLIT, $0-80
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ result_base+48(FP), DX
	MOVQ a_len+8(FP), BX

loop:
	CMPQ    BX, $0x00000008
	JL      done
	VMOVUPS (AX), Y0
	VMOVUPS (CX), Y1
	VADDPS  Y1, Y0, Y0
	VMOVUPS Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     loop

done:
	MOVQ BX, ret+72(FP)
	VZEROUPPER
	RET

// func f32x8_sub(a []float32, b []float32, result []float32) int
// Requires: AVX
TEXT ·f32x8_sub(SB), NOSPLIT, $0-80
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ result_base+48(FP), DX
	MOVQ a_len+8(FP), BX

loop:
	CMPQ    BX, $0x00000008
	JL      done
	VMOVUPS (AX), Y0
	VMOVUPS (CX), Y1
	VSUBPS  Y1, Y0, Y0
	VMOVUPS Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     loop

done:
	MOVQ BX, ret+72(FP)
	VZEROUPPER
	RET

// func f32x8_mul(a []float32, b []float32, result []float32) int
// Requires: AVX
TEXT ·f32x8_mul(SB), NOSPLIT, $0-80
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ result_base+48(FP), DX
	MOVQ a_len+8(FP), BX

loop:
	CMPQ    BX, $0x00000008
	JL      done
	VMOVUPS (AX), Y0
	VMOVUPS (CX), Y1
	VMULPS  Y1, Y0, Y0
	VMOVUPS Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     loop

done:
	MOVQ BX, ret+72(FP)
	VZEROUPPER
	RET

// func f32x8_div(a []float32, b []float32, result []float32) int
// Requires: AVX
TEXT ·f32x8_div(SB), NOSPLIT, $0-80
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ result_base+48(FP), DX
	MOVQ a_len+8(FP), BX

loop:
	CMPQ    BX, $0x00000008
	JL      done
	VMOVUPS (AX), Y0
	VMOVUPS (CX), Y1
	VDIVPS  Y1, Y0, Y0
	VMOVUPS Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     loop

done:
	MOVQ BX, ret+72(FP)
	VZEROUPPER
	RET

// func f32x8_and(a []float32, b []float32, result []float32) int
// Requires: AVX
TEXT ·f32x8_and(SB), NOSPLIT, $0-80
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ result_base+48(FP), DX
	MOVQ a_len+8(FP), BX

loop:
	CMPQ    BX, $0x00000008
	JL      done
	VMOVUPS (AX), Y0
	VMOVUPS (CX), Y1
	VANDPS  Y1, Y0, Y0
	VMOVUPS Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     loop

done:
	MOVQ BX, ret+72(FP)
	VZEROUPPER
	RET

// func f32x8_or(a []float32, b []float32, result []float32) int
// Requires: AVX
TEXT ·f32x8_or(SB), NOSPLIT, $0-80
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ result_base+48(FP), DX
	MOVQ a_len+8(FP), BX

loop:
	CMPQ    BX, $0x00000008
	JL      done
	VMOVUPS (AX), Y0
	VMOVUPS (CX), Y1
	VORPS   Y1, Y0, Y0
	VMOVUPS Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     loop

done:
	MOVQ BX, ret+72(FP)
	VZEROUPPER
	RET

// func f32x8_xor(a []float32, b []float32, result []float32) int
// Requires: AVX
TEXT ·f32x8_xor(SB), NOSPLIT, $0-80
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ result_base+48(FP), DX
	MOVQ a_len+8(FP), BX

loop:
	CMPQ    BX, $0x00000008
	JL      done
	VMOVUPS (AX), Y0
	VMOVUPS (CX), Y1
	VXORPS  Y1, Y0, Y0
	VMOVUPS Y0, (DX)
	ADDQ    $0x00000020, AX
	ADDQ    $0x00000020, CX
	ADDQ    $0x00000020, DX
	SUBQ    $0x00000008, BX
	JMP     loop

done:
	MOVQ BX, ret+72(FP)
	VZEROUPPER
	RET
